# GPT-2 Customer Support Chatbot (LoRA Fine-Tuning)

This project demonstrates a complete, end-to-end pipeline for fine-tuning a GPT-2 model to act as a customer support agent. It uses LoRA (Low-Rank Adaptation) via the Hugging Face `peft` library for efficient, low-resource training.

The workflow covers every step: from initial data conversion and inspection to tokenization, training, and finally, interactive inference.

## ‚ú® Features

* **End-to-End Pipeline:** All scripts necessary to go from a raw JSON dataset to a fully-trained chatbot are included.
* **Efficient Fine-Tuning:** Uses **LoRA (Low-Rank Adaptation)** via `peft` to significantly reduce the number of trainable parameters, allowing for faster training on consumer hardware.
* **Data Quality Assurance:** Includes scripts to sample the dataset (`better_dataset.py`) and inspect it for contamination, PII, and spam (`inspect_dataset.py`).
* **Correct Tokenization:** The `prepare_dataset.py` script correctly handles GPT-2's padding token (`pad_token = eos_token`) and formats the data as `<prompt> <response> <EOS>` for effective Causal LM training.
* **Optimized Training:** The `train_lora_gpt2.py` script is pre-configured with robust `TrainingArguments`, including gradient accumulation, cosine learning rate scheduler, and mixed-precision (FP16) support.
* **Ready-to-Use Inference:** `chat_with_model.py` shows how to load the final PEFT model and generate responses with best-practice sampling parameters (temperature, top-p, repetition penalty).

---

## üìÇ Project Structure

Your project directory should be set up as follows for the scripts to run correctly.

```

.
‚îú‚îÄ‚îÄ dataset/
‚îÇ   ‚îú‚îÄ‚îÄ mo\_customer\_support.json   \# \<-- Your initial raw dataset
‚îÇ   ‚îú‚îÄ‚îÄ final\_dataset.jsonl        \# (Generated by convert\_mo\_dataset.py)
‚îÇ   ‚îú‚îÄ‚îÄ better\_dataset.jsonl       \# (Generated by better\_dataset.py)
‚îÇ   ‚îú‚îÄ‚îÄ tokenizer/                 \# (Generated by prepare\_dataset.py)
‚îÇ   ‚îî‚îÄ‚îÄ tokenized\_customer\_support/  \# (Generated by prepare\_dataset.py)
‚îÇ
‚îú‚îÄ‚îÄ outputs/
‚îÇ   ‚îú‚îÄ‚îÄ customer\_support\_lora/       \# (Checkpoints from train\_lora\_gpt2.py)
‚îÇ   ‚îî‚îÄ‚îÄ final\_customer\_support\_model/  \# (Final model from train\_lora\_gpt2.py)
‚îÇ
‚îú‚îÄ‚îÄ convert\_mo\_dataset.py
‚îú‚îÄ‚îÄ better\_dataset.py
‚îú‚îÄ‚îÄ inspect\_dataset.py
‚îú‚îÄ‚îÄ prepare\_dataset.py
‚îú‚îÄ‚îÄ train\_lora\_gpt2.py
‚îú‚îÄ‚îÄ chat\_with\_model.py
‚îî‚îÄ‚îÄ README.md

````

---

## üöÄ Getting Started

### 1. Installation

First, clone the repository and install the required Python libraries.

```bash
# Install PyTorch (visit pytorch.org for CUDA-specific commands)
pip install torch

# Install core libraries
pip install transformers datasets peft pandas
````

### 2\. Initial Data

Place your original, raw dataset into the `dataset/` folder and name it `mo_customer_support.json`. The scripts assume this file exists.

> **Note:** The `convert_mo_dataset.py` script expects this file to be a JSON list of objects, where each object has an `"input"` (the customer's query) and an `"output"` (the agent's response).

-----

## üèÉ‚Äç‚ôÇÔ∏è Step-by-Step Workflow

Follow these steps in order to process your data, train the model, and test it.

### Step 1: Convert Raw Dataset

This script converts your initial `.json` file into the `.jsonl` format required for streaming and formats the text into a `prompt`/`response` structure.

```bash
python convert_mo_dataset.py
```

  * **Input:** `dataset/mo_customer_support.json`
  * **Output:** `dataset/final_dataset.jsonl`

### Step 2: Sample the Dataset

The full dataset might be too large. This script samples it down to a more manageable size (default: 30,000 examples) for faster training.

```bash
python better_dataset.py
```

  * **Input:** `dataset/final_dataset.jsonl`
  * **Output:** `dataset/better_dataset.jsonl`

### Step 3: (Optional) Inspect the Data

Before training, it's crucial to check your data for quality issues. This script looks for spam, PII, URLs, and other contamination.

```bash
python inspect_dataset.py
```

  * **Input:** `dataset/better_dataset.jsonl`
  * **Output:** Prints a quality report to the console. If issues are found, it saves samples to `dataset/contaminated_examples.jsonl` for review.

### Step 4: Tokenize the Dataset

This script loads the 'gpt2' tokenizer, configures it correctly, tokenizes all examples from the sampled dataset, and saves the processed data to disk.

```bash
python prepare_dataset.py
```

  * **Input:** `dataset/better_dataset.jsonl`
  * **Outputs:**
      * `dataset/tokenizer/` (The saved tokenizer)
      * `dataset/tokenized_customer_support/` (The tokenized dataset)

### Step 5: Train the LoRA Model

This is the main training script. It loads the base 'gpt2' model, applies a LoRA configuration, and fine-tunes it on your tokenized data.

```bash
python train_lora_gpt2.py
```

  * **Inputs:**
      * `dataset/tokenizer/`
      * `dataset/tokenized_customer_support/`
  * **Outputs:**
      * `outputs/customer_support_lora/` (Training checkpoints)
      * `outputs/final_customer_support_model/` (The final, trained LoRA model and tokenizer)

### Step 6: Chat with Your Model

After training, run this script to load your fine-tuned model and test it with sample prompts.

```bash
python chat_with_model.py
```

  * **Input:** `outputs/final_customer_support_model/`
  * **Output:** Prints generated agent responses to the console.

-----

## üìú Script Descriptions

  * **`convert_mo_dataset.py`**: Converts the initial `mo_customer_support.json` file into `final_dataset.jsonl`. It formats each entry into `{"prompt": "Customer: ...\nAgent:", "response": "..."}`.
  * **`better_dataset.py`**: Samples the `final_dataset.jsonl` to create `better_dataset.jsonl`. You can change the `sample_size` variable inside this file.
  * **`inspect_dataset.py`**: A utility script to analyze `better_dataset.jsonl`. It checks for data contamination (URLs, @mentions, PII), analyzes response lengths, and flags potential spam, providing a "go/no-go" for training.
  * **`prepare_dataset.py`**: Loads the `better_dataset.jsonl`, initializes the 'gpt2' tokenizer (and saves it to `dataset/tokenizer`), sets `pad_token` to `eos_token`, and tokenizes the data. It saves the final tokenized dataset to `dataset/tokenized_customer_support`.
  * **`train_lora_gpt2.py`**: The core training script. It loads the base 'gpt2' model, applies a LoRA configuration from `peft`, and trains the model using the `Trainer` API. It includes evaluation, checkpointing, and saves the final model.
  * **`chat_with_model.py`**: An inference script. It loads the final trained PEFT model from `outputs/final_customer_support_model` and generates responses for a list of test prompts.